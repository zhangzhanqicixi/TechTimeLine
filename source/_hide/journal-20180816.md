---
title: Spark 和 Deep Learning 来了
date: 2018-08-16 23:07:55
tags:
---

> 这篇是日记，日记我一般写在 Day One，但是因为感觉今天在工作上遇到了好多坑，所以过来同步记录一下。

今天状态很不错，早上在公司的服务器上终于跑通了 Max Compute 到 Spark 的调试。顺便设置了 Pycharm 直接 remote 到服务器，可以直接 auto commit，等于说大大提高了工作效率。

<!--more-->

阿里云提供的 MaxCompute - Spark 的包简直太坑了，由于它底层依赖了 org/apache/spark/logging 的包，但是这个包在 Spark 1.5 之后就没有了，所以我本地的 Spark 2.0 搞了半天跑不了，发现版本问题后无奈只能转到服务器再装一遍 Spark 1.5。降到 Spark 1.5 ，Hadoop 也要跟着降到 2.7，然后很多 pyspark 的 API 在 Spark 1.5 上都不能用，比如说 Spark 1.5 根本就没有 SparkSession，ALS 训练出来的 Model 最后的 recommendProductsForUsers 方法也在 Spark 1.5 内不提供，只能使用 recommendProducts 然后自己去遍历 User 一个个去 Predict，不知道会不会影响性能。再者还是没搞明白 Spark 1.5 内怎么 read 和 write csv 文件，感觉要依赖 scala 的 spark-csv jar 包，但是我把 jar 包导进去提示又没用，搞得我都想转 java 平台了。
Spark 2.0 直接很简单的使用 SparkSession 的实例然后 .read.csv(path) 就可以，这个 1.5 太麻烦了，只想说都是阿里惹的祸！！！
不过是不是反向说明 Spark 的开发还是在 Java 或 Scala 这些能依赖 Maven 的环境下比较好呢？因为我现在每次 submit 任务的时候真的带了好多 jar 包。。不知道那些用 pyspark 开发的同学是怎么搞的。

总之整体流程终于跑通了，csv 的问题暂时搁浅了，因为数据本来就不会通过本地上传的方式，以后肯定都是直接从 ODPS 或 Log Service 拿，csv 的问题暂时写在 Things 上吧。

这礼拜模仿 CSDN 的一篇博客，写个 Spark 1.5 和 Hadoop 2.7.2 的安装教程吧，顺便再把 HDFS 熟悉一下。
然后把 Spark 过程中遇到的问题，都放到了 Things 上，等啥时候也可以整理一下。

晚上开始花了 400 大洋上小象学院的「深度学习」的课程了，为了赶直播直接在开车的时候就听起来了，但是第一节课讲的没什么内容，下周再听一节看看，如果还可以的话也可以通过文字的形式同步到这里来。
总之，还是要让自己忙起来，这样才不会老是去想那些浪费时间的事情。

明天周五了，七夕节，准备把今天找 Leader 说的那部分现有的推荐逻辑看一下，看能不能看懂，如果看懂的话直接拉一部分数据用 Spark 跑跑看，但是误差计算还是要和他确认一下，现在 demo 用的是 RMSE。一切 ok 的话，就可以申请机子，正式的搭 Spark yarn 集群了。

咦，那我要不要把基于 yarn 的 Spark 搭出来之后再写 Spark 的博客呢？

![deeplearning](https://timeline229-image.oss-cn-hangzhou.aliyuncs.com/journal-20180816/deeplearning.jpg)